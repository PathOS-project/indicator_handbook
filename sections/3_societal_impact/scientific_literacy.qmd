---
author:
    - name: T. Klebel
      orcid: 0000-0002-7331-4751
      affiliations:
      - ref: know
    - name: E. Kormann
      orcid: 0009-0005-5680-3659
      affiliations:
      - ref: tug

affiliations:
- id: know
  name: Know-Center
  city: Graz
  country: Austria
- id: tug
  name: Graz University of Technology
  city: Graz
  country: Austria

title: Scientific literacy
---


::: {.callout collapse="true"}


# History

| Version | Revision date | Revision | Author |
|----|----|----|----|
| 1.0 | 2024-12-06 | General revisions | Thomas Klebel |
| 0.3 | 2024-12-03 | Extending sections on measurement | Eva Kormann |
| 0.2 | 2024-02-27 | Review comments | Nicki Lisa Cole, Vincent Traag, Ioanna Grypari, Tommaso Venturi |
| 0.1 | 2024-02-02 | First draft | Thomas Klebel |

:::

# Description

Scientific literacy is a concept aimed at measuring an individual's ability to engage with and understand scientific concepts and discussions. Despite the extensive literature on the subject, there is no common definition [@deboer2000; @laugksch2000; @norris2014; @roberts2007]. Science literacy is generally seen as a desirable goal, with diverging conceptions on what elements of it are desirable [@deboer2000; @laugksch2000], and for which reasons [@norris2014]. An emergent topic in the literature is to what extent science education (and thus, scientific literacy) currently provides a sufficient response to the "age of misinformation" [@feinstein2020; @osborne2023]. Three perspectives on the concept of scientific literacy are instructive.

First, in a view proposed by @laugksch2000 and supported by @roberts2007, there are at least three stakeholder groups engaged with scientific literacy: (1) sociologists, (2) public opinion researchers, (3) science educators, including those involved in science communication. All three stakeholder groups approach the topic with their own goals and justifications for why scientific literacy matters. Consequently, they also employ different methods to measure the concept, ranging from in-depth interviews to representative population samples, to assessments of students' competencies.

Second, @laugksch2000 and @norris2014 conceptualise scientific literacy to comprise of three different interpretations of what it means to be 'literate'. The first refers to what one has learned – the specific knowledge gained. The second refers to being competent, having a certain capacity to engage with scientific contents. The third refers to how scientific literacy might enable one "to function minimally in society" [@laugksch2000, p. 82], that is, being able to the live up to the expectations inherent to a certain role in society, such as being a competent citizen or consumer.

Third, @roberts2007 provides a useful distinction between two "visions" of scientific literacy. Here the term "vision" is broader than a mere definition and represents an ideal type in the Weberian sense—an intentionally accentuated construct designed to highlight its core features while serving as a conceptual tool for analysis. Vision I in Roberts' terms is concerned with literacy or knowledgeability *within science*, that is, it is targeted at an understanding of scientific products (publications, datasets, claims) and processes [@roberts2007, p. 730]. In contrast, vision II is targeted at situations where scientific knowledge can aid citizens in their daily lives: "At the extreme, this vision can be called *literacy* (again, read *thorough knowledgeability*) *about science-related situations* in which considerations other than science have an important place at the table" [@roberts2007, p. 730]. As an example, this vision is reflected in how the OECD defines scientific literacy in PISA [@roberts2007, p. 766]:

> PISA defines scientific literacy as the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen. PISA's definition includes being able to explain phenomena scientifically, evaluate and design scientific enquiry, and interpret data and evidence scientifically. *It emphasises the importance of being able to apply scientific knowledge in the context of real-life situations.* [@oecd2017b, own emphasis]

A more recent summary of the existing conceptualisations of scientific literacy, including the discussion of a third "vision", can be found in @valladares2021.

Given the diverse perspectives and the lack of consensus on a definition, metrics to study scientific literacy should be chosen and evaluated against the specific goal of a particular study [@laugksch2000, p. 88]. @coppi2023 provide an overview of existing measurement tools and their applications. Below, we highlight key examples of how scientific literacy is commonly assessed.

The Programme for International Student Assessment (PISA) Assessment and Analytical Framework defines scientific literacy as "the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen" [@oecd2017a, p. 15], and includes a description on how it is measured. However, specific questions or results from the survey on scientific literacy are not publicly available.

The three competencies that PISA includes in scientific literacy are (1) explaining phenomena scientifically, (2) evaluating and designing scientific inquiry and (3) scientifically interpreting data and evidence, which all require knowledge (content, procedural, and epistemic). Competencies and knowledge, together with context and attitudes, make up the four interrelated aspects within the PISA 2015 scientific literacy assessment framework.

While the PISA instrument is not publicly available, the framework includes detailed descriptions of the proportions at which different competencies, types of knowledge, depths of knowledge and contexts should be covered by the assessment items. Exemplary items contain scenarios that require students for example to assess presented evidence, understand data and apply content knowledge, while responding to simple or complex multiple-choice questions or constructed response items. For the final scoring, students are placed on a seven-level proficiency scale based on the responses to individual items [@oecd2017a].

Besides the PISA instrument, there is a great variety of other instruments used to assess scientific literacy, commonly aligned with vision II and the PISA conceptualization. In general, the focus seems to have shifted from viewing scientific literacy as one single ability to defining it as a multidimensional construct. Most of the instruments' contents focus on a specific domain or context (e.g., biology), employing various formats to test skills. The majority of instruments have been developed for use in (secondary school) students and have mainly been employed in that context [@coppi2023; @opitz2017].

A relatively recent and broadly used instrument is the Test of Scientific Literacy Skills (TOSLS), developed to be a comprehensive and psychometrically sound measure that can be employed on a large scale. The TOSLS includes 28 multiple-choice questions presented through realistic scenarios and requiring respondents to use skills that relate to understanding scientific methods and interpreting data [@gormally2012]. The instrument is available as the supplement to @gormally2012.

# Operationalization in Case Studies

## PathOS Case Studies

This indicator has been operationalized in the following case studies:
- French Open Access Infrastructure

Further details are available in Deliverable D3.3 of PathOS, accessible through the [PathOS Zenodo community](https://zenodo.org/communities/pathos/records?q=&l=list&p=1&s=10&sort=newest).
